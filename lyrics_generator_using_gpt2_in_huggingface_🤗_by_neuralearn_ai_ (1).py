# -*- coding: utf-8 -*-
"""Lyrics Generator using GPT2 in HuggingFace ðŸ¤— by Neuralearn.ai--.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OuzOIsagI1Sgmg5GYCxZKo0xk8Uo69La

# Installations
"""

!pip install transformers datasets

import tensorflow as tf### models
import numpy as np### math computations
import matplotlib.pyplot as plt### plotting bar chart
import sklearn### machine learning library
import cv2## image processing
from sklearn.metrics import confusion_matrix, roc_curve### metrics
import seaborn as sns### visualizations
import datetime
import pathlib
import io
import os
import re
import string
import time
from numpy import random
import gensim.downloader as api
from PIL import Image
import tensorflow_datasets as tfds
import tensorflow_probability as tfp
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Dense,Flatten,InputLayer,BatchNormalization,Dropout,Input,LayerNormalization
from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy
from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy
from tensorflow.keras.optimizers import Adam
from google.colab import drive
from google.colab import files
from datasets import load_dataset
from transformers import GPT2TokenizerFast,create_optimizer,DataCollatorForLanguageModeling,TFGPT2LMHeadModel

MAX_LENGTH=256
BATCH_SIZE=6

"""# Dataset Preparation"""

!pip install -q kaggle
!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json
!kaggle datasets download -d juicobowley/drake-lyrics
!unzip "/content/drake-lyrics.zip" -d "/content/dataset/"

filepath="/content/dataset/drake_data.csv"
dataset = load_dataset('csv', data_files=filepath)

dataset

dataset['train'][184]

model_id="gpt2-medium"
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

dataset['train'][184]

n_wasted=0

# for i in range(len(dataset['train'])):
#   try:
#     outputs = tokenizer(
#       dataset["train"][i]["lyrics"],
#       truncation=True,
#       max_length=256,
#       return_overflowing_tokens=True,
#       return_length=True,
#     )
#     print(i,outputs['length'])

#     for k in outputs['length']:
#       if k!=256:
#         n_wasted+=k
#   except:
#     print('----------------------->i',i)

print(n_wasted)

def preprocess_function(example):
  try:
    outputs = tokenizer(
        example["lyrics"],
        truncation=True,
        max_length=MAX_LENGTH,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
      if length==MAX_LENGTH:
        input_batch.append(input_ids)
        valid_input_ids=input_ids
    if len(input_batch)!=0:
      for i in range(BATCH_SIZE-len(input_batch)):
        input_batch.append(valid_input_ids)
  except:
    print(example)
    input_batch=[]
  return {"input_ids": input_batch}

tokenized_dataset=dataset.map(
    preprocess_function,remove_columns=dataset["train"].column_names
)

tokenized_dataset

def filter_out(example):
  if len(example['input_ids'])>=1:
    return example

tokenized_full_dataset=tokenized_dataset.filter(filter_out)
print(tokenized_full_dataset)

max_batch_len=0

for i in range(270):
  if len(tokenized_full_dataset['train'][i]['input_ids'])>max_batch_len:
    max_batch_len=len(tokenized_full_dataset['train'][i]['input_ids'])
  #print(i,len(tokenized_full_dataset['train'][i]['input_ids']))

print(max_batch_len)

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")

tf_train_dataset = tokenized_full_dataset["train"].to_tf_dataset(
    columns=["input_ids","attention_mask", "labels"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=1,
)

for i in tf_train_dataset.take(1):
  print(i)

def adjust_attention_mask(input):
  return {'input_ids':input['input_ids'],
          'attention_mask':tf.ones([1,BATCH_SIZE,MAX_LENGTH]),
          'labels':input['labels']}

train_dataset=tf_train_dataset.map(adjust_attention_mask)

for i in train_dataset.take(1):
  print(i)

unbatched_dataset=train_dataset.unbatch()

for i in unbatched_dataset.take(1):
  print(i)

"""# Modeling"""

model = TFGPT2LMHeadModel.from_pretrained(model_id)
model.summary()

num_train_steps=len(unbatched_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
)
model.compile(optimizer=optimizer)

history=model.fit(unbatched_dataset, epochs=5)

model.save_weights('/content/drive/MyDrive/nlp/text_generation/gpt2_medium.h5')

input_text="true love shouldn't be this complicated"

input_ids = tokenizer(input_text, return_tensors="tf")["input_ids"]

init_time=time.time()
output_greedy = model.generate(input_ids,max_length=256,do_sample=False)
print(tokenizer.decode(output_greedy[0]))
print(time.time()-init_time)

init_time=time.time()
output_beam = model.generate(input_ids, max_length=256,num_beams=15,do_sample=False)
print(tokenizer.decode(output_beam[0]))
print(time.time()-init_time)

init_time=time.time()
output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=1.0, top_k=0)
print(tokenizer.decode(output_temp[0]))
print(time.time()-init_time)

init_time=time.time()
output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=2.0, top_k=0)
print(tokenizer.decode(output_temp[0]))
print(time.time()-init_time)

init_time=time.time()
output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))
print(time.time()-init_time)

init_time=time.time()
output_topk = model.generate(input_ids, max_length=256, do_sample=True,top_k=50)
print(tokenizer.decode(output_topk[0]))
print(time.time()-init_time)

init_time=time.time()
output_topk = model.generate(input_ids, max_length=256, do_sample=True,temperature=2.0,top_k=50)
print(tokenizer.decode(output_topk[0]))
print(time.time()-init_time)

init_time=time.time()
output_topp = model.generate(input_ids, max_length=256, do_sample=True,top_p=0.90)
print(tokenizer.decode(output_topp[0]))
print(time.time()-init_time)

conclusion
which method produces best results,
compare summarization and other tasks

























input_ids = tokenizer(input_text, return_tensors="tf")["input_ids"]
output_greedy = model.generate(input_ids,max_length=256,do_sample=False)
print(tokenizer.decode(output_greedy[0]))

output_beam = model.generate(input_ids, max_length=256,num_beams=5,do_sample=False)
print(tokenizer.decode(output_beam[0]))

output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=2.0, top_k=0)
print(tokenizer.decode(output_temp[0]))

output_temp = model.generate(input_ids, max_length=256, do_sample=True,temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))

output_topk = model.generate(input_ids, max_length=256, do_sample=True,top_k=50)
print(tokenizer.decode(output_topk[0]))

output_topp = model.generate(input_ids, max_length=256, do_sample=True,top_p=0.90)
print(tokenizer.decode(output_topp[0]))







































history=model.fit(unbatched_dataset, epochs=10)

from transformers import pipeline

pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer, max_length=256,
)

txt="I put my knee on the floor, baby please open the door, it's getting rough on me, someone please come for me"

print(pipe(input_text, num_return_sequences=1)[0]["generated_text"])



print(pipe(input_text, num_return_sequences=1)[0]["generated_text"])